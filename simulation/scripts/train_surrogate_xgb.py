#!/usr/bin/env python3
"""
train_surrogate_xgb.py
----------------------

Train an XGBoost regression surrogate model to predict the
coherent edge (peak_energy) using simulation data.

Assumes input CSV was generated by xgboost_generate_data.py
or run_batch.py and contains the physics variables plus
dp, dy, run-length, etc.

Output:
    - trained model saved as xgb_model.json
    - printed metrics: RMSE, MAE, R^2
"""

import argparse
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score


def main():

    parser = argparse.ArgumentParser(description="Train XGBoost surrogate model")
    parser.add_argument("--data", type=str, required=True,
                        help="Path to training CSV (e.g. xgb_train.csv)")
    parser.add_argument("--model_out", type=str, default="xgb_model.json",
                        help="File to save trained model")
    parser.add_argument("--test_size", type=float, default=0.2,
                        help="Fraction of data to use for validation")
    args = parser.parse_args()

    print(f"\nLoading data from: {args.data}")
    df = pd.read_csv(args.data)

    # ============================================================
    # Target and input feature selection
    # ============================================================

    # Target variable
    y = df["peak_energy"]     # predict the coherent edge

    # Drop non-feature variables
    drop_cols = [
        "peak_energy",  # target
        "episode", "step"   # metadata not useful for prediction
    ]

    # Input features: everything except drop_cols
    X = df.drop(columns=drop_cols)

    print(f"Dataset shape: {df.shape}")
    print(f"Feature count: {X.shape[1]}")

    # ============================================================
    # Train/test split
    # ============================================================
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=args.test_size, random_state=42, shuffle=True
    )

    print(f"\nTraining samples:   {X_train.shape[0]}")
    print(f"Validation samples: {X_test.shape[0]}")

    # ============================================================
    # XGBoost regression model
    # ============================================================
    model = xgb.XGBRegressor(
        n_estimators=500,
        learning_rate=0.05,
        max_depth=8,
        subsample=0.9,
        colsample_bytree=0.9,
        objective="reg:squarederror",
        tree_method="hist",   # fast
        random_state=42
    )

    print("\nTraining XGBoost model...")
    model.fit(X_train, y_train)

    # ============================================================
    # Evaluate
    # ============================================================
    y_pred = model.predict(X_test)

    rmse = mean_squared_error(y_test, y_pred, squared=False)
    mae  = mean_absolute_error(y_test, y_pred)
    r2   = r2_score(y_test, y_pred)

    print("\n================= SURROGATE MODEL PERFORMANCE =================")
    print(f"RMSE: {rmse:.6f} MeV")
    print(f"MAE:  {mae:.6f} MeV")
    print(f"R^2:  {r2:.6f}")
    print("================================================================")

    # ============================================================
    # Save model
    # ============================================================
    model.save_model(args.model_out)
    print(f"\nModel saved to: {args.model_out}\n")


if __name__ == "__main__":
    main()
